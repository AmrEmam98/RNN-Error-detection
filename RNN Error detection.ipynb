{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbTKAB6s9BfI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AM4kATx_3Fv"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('word_annotation_2m.csv')\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['word'], df['label'],stratify=df['label'], test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2eD352YAXT_"
      },
      "outputs": [],
      "source": [
        "all_letters='بتثجحخدذرزسشصضطظعغفقكلمنهويءآٱأإةؤئىئ '\n",
        "n_letters=len(all_letters)\n",
        "train_data=list(zip(X_train,y_train))\n",
        "test_data=list(zip(X_test,y_test))\n",
        "device=torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmQS_jffNOLj"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size,):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size).to(device)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size).to(device)\n",
        "        self.sigmoid = nn.Sigmoid().to(device)\n",
        "          \n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1).to(device)\n",
        "        hidden = self.i2h(combined).to(device)\n",
        "        output = self.i2o(combined).to(device)\n",
        "        output = self.sigmoid(output).to(device)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHrpJy5n6Kji"
      },
      "outputs": [],
      "source": [
        "class ErrorDetectionModel:\n",
        "  def __init__(self,input_size=50,hidden_size=128, output_size=1,ckp_path=None,train_dataloader=None,valid_dataloader=None):\n",
        "    self.rnn = RNN(n_letters, hidden_size, output_size).to(device)\n",
        "    self.criterion = nn.BCELoss()\n",
        "    #load last_ckp\n",
        "    if ckp_path:\n",
        "      print('loading checkpoint saved at ',ckp_path)\n",
        "      self.rnn.load_state_dict(torch.load(ckp_path))\n",
        "    #set data\n",
        "    self.train_dataloader=train_dataloader\n",
        "    self.valid_dataloader=valid_dataloader\n",
        "\n",
        "  def labelFromOutput(self,output):\n",
        "    return int(output > 0.5)\n",
        "\n",
        "  def save_checkpoint(self,path):\n",
        "    torch.save(self.rnn.state_dict(), path)\n",
        "\n",
        "  def predict(self,word_tensor_batch):\n",
        "    output_batch_tensor=torch.zeros(len(word_tensor_batch),1).to(device)\n",
        "    for idx, word_tensor in enumerate(word_tensor_batch):\n",
        "      hidden = self.rnn.initHidden()\n",
        "      for i in range(word_tensor.size()[0]):\n",
        "          output, hidden = self.rnn(word_tensor[i], hidden)\n",
        "      output_batch_tensor[idx]=output\n",
        "    return output_batch_tensor\n",
        "  \n",
        "  def evaluate(self,test_dataloader):\n",
        "    all_predictions=[]\n",
        "    loss=0\n",
        "    for words_batch , label_batch, label_tensor_batch, word_tensor_batch  in tqdm(test_dataloader):\n",
        "      pred_batch = self.predict(word_tensor_batch)\n",
        "      loss += self.criterion(pred_batch, label_tensor_batch)\n",
        "      predicted_labels=[int(label==self.labelFromOutput(pred) ) for label, pred in zip(label_tensor_batch,pred_batch)]\n",
        "      all_predictions+=predicted_labels\n",
        "    avg_loss=loss/len(test_dataloader)\n",
        "    acc=np.mean(all_predictions)\n",
        "    return acc , avg_loss.item()\n",
        "\n",
        "\n",
        "  def train(self,epochs=10,learning_rate=0.001,save_every=1):\n",
        "    self.rnn.train()\n",
        "    all_losses=[]\n",
        "    min_valid_loss=1000\n",
        "    optimizer = torch.optim.Adam(self.rnn.parameters(), lr=learning_rate)\n",
        "    for epoch in range(1,epochs+1):\n",
        "      current_loss=0\n",
        "      for words_batch , label_batch, label_tensor_batch, word_tensor_batch  in tqdm(self.train_dataloader):\n",
        "        output_batch_tensor=self.predict(word_tensor_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss = self.criterion(output_batch_tensor, label_tensor_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss+=loss.item()\n",
        "        \n",
        "      current_loss/=len(train_dataloader)\n",
        "      all_losses.append(current_loss)\n",
        "\n",
        "      if self.valid_dataloader:\n",
        "        acc , current_valid_loss=self.evaluate(self.valid_dataloader)\n",
        "        print('Epoch: {} | valid Loss= {:.3f}  | valid accuracy= {:.3f}'.format(epoch,current_valid_loss,acc))\n",
        "        if current_valid_loss  < min_valid_loss:\n",
        "          min_valid_loss=current_valid_loss\n",
        "          self.save_checkpoint('ckp_best.pt')\n",
        "\n",
        "      if epoch % save_every ==0 :\n",
        "          self.save_checkpoint('ckp_{}.pt'.format(epoch))\n",
        "      print('Epoch: {} | train loss= {:.3f}'.format(epoch,current_loss))\n",
        "      self.save_checkpoint('ckp_last.pt'.format(epoch))\n",
        "    return all_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-72USkqUEXCJ"
      },
      "outputs": [],
      "source": [
        "class WordsDataset(Dataset):\n",
        "\n",
        "    def __init__(self,data,max_len=16):\n",
        "      self.data=data\n",
        "      self.max_len=max_len\n",
        "\n",
        "    def letterToIndex(self,letter):\n",
        "        return all_letters.find(letter)\n",
        "\n",
        "    def letterToTensor(self,letter):\n",
        "        tensor = torch.zeros(1, n_letters)\n",
        "        tensor[0][self.letterToIndex(letter)] = 1\n",
        "        return tensor\n",
        "\n",
        "    def lineToTensor(self,line):\n",
        "        tensor = torch.zeros(self.max_len, 1, n_letters)\n",
        "        for i in range(min(self.max_len,len(line))):\n",
        "            tensor[i][0][self.letterToIndex(line[i])] = 1\n",
        "        return tensor.to(device)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line , label = self.data[idx]\n",
        "        line_tensor = self.lineToTensor(line)\n",
        "        label_tensor=torch.tensor([label], dtype=torch.float).to(device)\n",
        "        return   line , label, label_tensor, line_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PDhvVufLEeW"
      },
      "outputs": [],
      "source": [
        "training_data=WordsDataset(train_data)\n",
        "test_dataset=WordsDataset(test_data)\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXwqRmBrA1LM"
      },
      "outputs": [],
      "source": [
        "model=ErrorDetectionModel(input_size=n_letters,hidden_size=32,train_dataloader=train_dataloader,valid_dataloader=test_dataloader)\n",
        "model.train()\n",
        "model.evaluate(test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}